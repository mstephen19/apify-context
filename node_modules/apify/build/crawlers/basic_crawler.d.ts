/**
 * @typedef BasicCrawlerOptions
 * @property {HandleRequest} handleRequestFunction
 *   User-provided function that performs the logic of the crawler. It is called for each URL to crawl.
 *
 *   The function receives the following object as an argument:
 * ```
 * {
 *   request: Request,
 *   session: Session,
 *   crawler: BasicCrawler,
 * }
 * ```
 *   where the {@link Request} instance represents the URL to crawl.
 *
 *   The function must return a promise, which is then awaited by the crawler.
 *
 *   If the function throws an exception, the crawler will try to re-crawl the
 *   request later, up to `option.maxRequestRetries` times.
 *   If all the retries fail, the crawler calls the function
 *   provided to the `handleFailedRequestFunction` parameter.
 *   To make this work, you should **always**
 *   let your function throw exceptions rather than catch them.
 *   The exceptions are logged to the request using the
 *   {@link Request#pushErrorMessage} function.
 * @property {RequestList} [requestList]
 *   Static list of URLs to be processed.
 *   Either `requestList` or `requestQueue` option must be provided (or both).
 * @property {RequestQueue} [requestQueue]
 *   Dynamic queue of URLs to be processed. This is useful for recursive crawling of websites.
 *   Either `requestList` or `requestQueue` option must be provided (or both).
 * @property {number} [handleRequestTimeoutSecs=60]
 *   Timeout in which the function passed as `handleRequestFunction` needs to finish, in seconds.
 * @property {HandleFailedRequest} [handleFailedRequestFunction]
 *   A function to handle requests that failed more than `option.maxRequestRetries` times.
 *
 *   The function receives the following object as an argument:
 * ```
 * {
 *   request: Request,
 *   error: Error,
 *   session: Session,
 *   crawler: BasicCrawler,
 * }
 * ```
 *   where the {@link Request} instance corresponds to the failed request, and the `Error` instance
 *   represents the last error thrown during processing of the request.
 *
 *   See
 *   [source code](https://github.com/apify/apify-js/blob/master/src/crawlers/basic_crawler.js#L11)
 *   for the default implementation of this function.
 * @property {number} [maxRequestRetries=3]
 *   Indicates how many times the request is retried if {@link BasicCrawlerOptions.handleRequestFunction} fails.
 * @property {number} [maxRequestsPerCrawl]
 *   Maximum number of pages that the crawler will open. The crawl will stop when this limit is reached.
 *   Always set this value in order to prevent infinite loops in misconfigured crawlers.
 *   Note that in cases of parallel crawling, the actual number of pages visited might be slightly higher than this value.
 * @property {AutoscaledPoolOptions} [autoscaledPoolOptions]
 *   Custom options passed to the underlying {@link AutoscaledPool} constructor.
 *   Note that the `runTaskFunction` and `isTaskReadyFunction` options
 *   are provided by `BasicCrawler` and cannot be overridden.
 *   However, you can provide a custom implementation of `isFinishedFunction`.
 * @property {number} [minConcurrency=1]
 *   Sets the minimum concurrency (parallelism) for the crawl. Shortcut to the corresponding {@link AutoscaledPool} option.
 *
 *   *WARNING:* If you set this value too high with respect to the available system memory and CPU, your crawler will run extremely slow or crash.
 *   If you're not sure, just keep the default value and the concurrency will scale up automatically.
 * @property {number} [maxConcurrency=1000]
 *   Sets the maximum concurrency (parallelism) for the crawl. Shortcut to the corresponding {@link AutoscaledPool} option.
 * @property {boolean} [useSessionPool=true]
 *   Basic crawler will initialize the  {@link SessionPool} with the corresponding `sessionPoolOptions`.
 *   The session instance will be than available in the `handleRequestFunction`.
 * @property {SessionPoolOptions} [sessionPoolOptions] The configuration options for {@link SessionPool} to use.
 */
/**
 * Provides a simple framework for parallel crawling of web pages.
 * The URLs to crawl are fed either from a static list of URLs
 * or from a dynamic queue of URLs enabling recursive crawling of websites.
 *
 * `BasicCrawler` is a low-level tool that requires the user to implement the page
 * download and data extraction functionality themselves.
 * If you want a crawler that already facilitates this functionality,
 * please consider using {@link CheerioCrawler}, {@link PuppeteerCrawler} or {@link PlaywrightCrawler}.
 *
 * `BasicCrawler` invokes the user-provided {@link BasicCrawlerOptions.handleRequestFunction}
 * for each {@link Request} object, which represents a single URL to crawl.
 * The {@link Request} objects are fed from the {@link RequestList} or the {@link RequestQueue}
 * instances provided by the {@link BasicCrawlerOptions.requestList} or {@link BasicCrawlerOptions.requestQueue}
 * constructor options, respectively.
 *
 * If both {@link BasicCrawlerOptions.requestList} and {@link BasicCrawlerOptions.requestQueue} options are used,
 * the instance first processes URLs from the {@link RequestList} and automatically enqueues all of them
 * to {@link RequestQueue} before it starts their processing. This ensures that a single URL is not crawled multiple times.
 *
 * The crawler finishes if there are no more {@link Request} objects to crawl.
 *
 * New requests are only dispatched when there is enough free CPU and memory available,
 * using the functionality provided by the {@link AutoscaledPool} class.
 * All {@link AutoscaledPool} configuration options can be passed to the `autoscaledPoolOptions`
 * parameter of the `BasicCrawler` constructor. For user convenience, the `minConcurrency` and `maxConcurrency`
 * {@link AutoscaledPool} options are available directly in the `BasicCrawler` constructor.
 *
 * **Example usage:**
 *
 * ```javascript
 * // Prepare a list of URLs to crawl
 * const requestList = new Apify.RequestList({
 *   sources: [
 *       { url: 'http://www.example.com/page-1' },
 *       { url: 'http://www.example.com/page-2' },
 *   ],
 * });
 * await requestList.initialize();
 *
 * // Crawl the URLs
 * const crawler = new Apify.BasicCrawler({
 *     requestList,
 *     handleRequestFunction: async ({ request }) => {
 *         // 'request' contains an instance of the Request class
 *         // Here we simply fetch the HTML of the page and store it to a dataset
 *         const { body } = await Apify.utils.requestAsBrowser(request);
 *         await Apify.pushData({
 *             url: request.url,
 *             html: body,
 *         })
 *     },
 * });
 *
 * await crawler.run();
 * ```
 * @property {Statistics} stats
 *  Contains statistics about the current run.
 * @property {RequestList} [requestList]
 *  A reference to the underlying {@link RequestList} class that manages the crawler's {@link Request}s.
 *  Only available if used by the crawler.
 * @property {RequestQueue} [requestQueue]
 *  A reference to the underlying {@link RequestQueue} class that manages the crawler's {@link Request}s.
 *  Only available if used by the crawler.
 * @property {SessionPool} [sessionPool]
 *  A reference to the underlying {@link SessionPool} class that manages the crawler's {@link Session}s.
 *  Only available if used by the crawler.
 * @property {AutoscaledPool} autoscaledPool
 *  A reference to the underlying {@link AutoscaledPool} class that manages the concurrency of the crawler.
 *  Note that this property is only initialized after calling the {@link BasicCrawler#run} function.
 *  You can use it to change the concurrency settings on the fly,
 *  to pause the crawler by calling {@link AutoscaledPool#pause}
 *  or to abort it by calling {@link AutoscaledPool#abort}.
 */
export class BasicCrawler {
    /**
     * @internal
     * @type any
     */
    static optionsShape: any;
    /**
     * @param {BasicCrawlerOptions} options
     * All `BasicCrawler` parameters are passed via an options object.
     */
    constructor(options: BasicCrawlerOptions);
    /** @type {defaultLog.Log} */
    log: any;
    requestList: RequestList | undefined;
    requestQueue: RequestQueue | undefined;
    userProvidedHandler: HandleRequest;
    failedContextHandler: HandleFailedRequest | undefined;
    handleRequestTimeoutMillis: number;
    internalTimeoutMillis: number;
    handleFailedRequestFunction: HandleFailedRequest | undefined;
    maxRequestRetries: number;
    handledRequestsCount: number;
    stats: Statistics;
    /** @type {SessionPoolOptions} */
    sessionPoolOptions: SessionPoolOptions;
    useSessionPool: boolean;
    crawlingContexts: Map<any, any>;
    autoscaledPoolOptions: any;
    isRunningPromise: Promise<void> | null;
    /**
     * Runs the crawler. Returns a promise that gets resolved once all the requests are processed.
     *
     * @return {Promise<void>}
     */
    run(): Promise<void>;
    /**
     * @return {Promise<void>}
     * @ignore
     * @protected
     * @internal
     */
    protected _init(): Promise<void>;
    autoscaledPool: AutoscaledPool | undefined;
    sessionPool: import("../session_pool/session_pool").SessionPool | undefined;
    /**
     * @param {CrawlingContext} crawlingContext
     * @return {Promise<void>}
     * @ignore
     * @protected
     * @internal
     */
    protected _handleRequestFunction(crawlingContext: CrawlingContext): Promise<void>;
    /**
     * @ignore
     * @protected
     * @internal
     */
    protected _pauseOnMigration(): Promise<void>;
    /**
     * Fetches request from either RequestList or RequestQueue. If request comes from a RequestList
     * and RequestQueue is present then enqueues it to the queue first.
     *
     * @ignore
     * @protected
     * @internal
     */
    protected _fetchNextRequest(): Promise<Request | null>;
    /**
     * Wrapper around handleRequestFunction that fetches requests from RequestList/RequestQueue
     * then retries them in a case of an error, etc.
     *
     * @ignore
     * @protected
     * @internal
     */
    protected _runTaskFunction(): Promise<void>;
    /**
     * Run async callback with given timeout and retry.
     * @ignore
     */
    _timeoutAndRetry(handler: any, timeout: any, error: any, maxRetries?: number, retried?: number): any;
    /**
     * Returns true if either RequestList or RequestQueue have a request ready for processing.
     *
     * @ignore
     * @protected
     * @internal
     */
    protected _isTaskReadyFunction(): Promise<boolean>;
    /**
     * Returns true if both RequestList and RequestQueue have all requests finished.
     *
     * @ignore
     * @protected
     * @internal
     */
    protected _defaultIsFinishedFunction(): Promise<boolean>;
    /**
     * Handles errors thrown by user provided handleRequestFunction()
     * @param {Error} error
     * @param {object} crawlingContext
     * @param {Request} crawlingContext.request
     * @param {(RequestList|RequestQueue)} source
     * @return {Promise<void>}
     * @ignore
     * @protected
     * @internal
     */
    protected _requestFunctionErrorHandler(error: Error, crawlingContext: {
        request: Request;
    }, source: (RequestList | RequestQueue)): Promise<void>;
    /**
     * @param {object} crawlingContext
     * @param {Error} crawlingContext.error
     * @param {Request} crawlingContext.request
     * @return {Promise<void>}
     * @ignore
     * @protected
     * @internal
     */
    protected _handleFailedRequestFunction(crawlingContext: {
        error: Error;
        request: Request;
    }): Promise<void>;
    /**
     * Updates handledRequestsCount from possibly stored counts,
     * usually after worker migration. Since one of the stores
     * needs to have priority when both are present,
     * it is the request queue, because generally, the request
     * list will first be dumped into the queue and then left
     * empty.
     *
     * @return {Promise<void>}
     * @ignore
     * @protected
     * @internal
     */
    protected _loadHandledRequestCount(): Promise<void>;
    /**
     * @param {Array<Hook>} hooks
     * @param  {*} args
     * @ignore
     * @protected
     * @internal
     */
    protected _executeHooks(hooks: Array<any>, ...args: any): Promise<void>;
    /**
     * Function for cleaning up after all request are processed.
     * @ignore
     */
    teardown(): Promise<void>;
}
export type CrawlingContext = {
    id: string;
    request: Request;
    session: Session;
    proxyInfo: ProxyInfo;
    response: any;
};
export type BasicCrawlerOptions = {
    /**
     *   User-provided function that performs the logic of the crawler. It is called for each URL to crawl.
     *
     *   The function receives the following object as an argument:
     * ```
     * {
     *   request: Request,
     *   session: Session,
     *   crawler: BasicCrawler,
     * }
     * ```
     *   where the {@link Request } instance represents the URL to crawl.
     *
     *   The function must return a promise, which is then awaited by the crawler.
     *
     *   If the function throws an exception, the crawler will try to re-crawl the
     *   request later, up to `option.maxRequestRetries` times.
     *   If all the retries fail, the crawler calls the function
     *   provided to the `handleFailedRequestFunction` parameter.
     *   To make this work, you should **always**
     *   let your function throw exceptions rather than catch them.
     *   The exceptions are logged to the request using the
     *   {@link RequestpushErrorMessage } function.
     */
    handleRequestFunction: HandleRequest;
    /**
     * Static list of URLs to be processed.
     * Either `requestList` or `requestQueue` option must be provided (or both).
     */
    requestList?: RequestList | undefined;
    /**
     * Dynamic queue of URLs to be processed. This is useful for recursive crawling of websites.
     * Either `requestList` or `requestQueue` option must be provided (or both).
     */
    requestQueue?: RequestQueue | undefined;
    /**
     * Timeout in which the function passed as `handleRequestFunction` needs to finish, in seconds.
     */
    handleRequestTimeoutSecs?: number | undefined;
    /**
     * A function to handle requests that failed more than `option.maxRequestRetries` times.
     *
     * The function receives the following object as an argument:
     * ```
     * {
     * request: Request,
     * error: Error,
     * session: Session,
     * crawler: BasicCrawler,
     * }
     * ```
     * where the {@link Request } instance corresponds to the failed request, and the `Error` instance
     * represents the last error thrown during processing of the request.
     *
     * See
     * [source code](https://github.com/apify/apify-js/blob/master/src/crawlers/basic_crawler.js#L11)
     * for the default implementation of this function.
     */
    handleFailedRequestFunction?: HandleFailedRequest | undefined;
    /**
     * Indicates how many times the request is retried if {@link BasicCrawlerOptions.handleRequestFunction } fails.
     */
    maxRequestRetries?: number | undefined;
    /**
     * Maximum number of pages that the crawler will open. The crawl will stop when this limit is reached.
     * Always set this value in order to prevent infinite loops in misconfigured crawlers.
     * Note that in cases of parallel crawling, the actual number of pages visited might be slightly higher than this value.
     */
    maxRequestsPerCrawl?: number | undefined;
    /**
     * Custom options passed to the underlying {@link AutoscaledPool } constructor.
     * Note that the `runTaskFunction` and `isTaskReadyFunction` options
     * are provided by `BasicCrawler` and cannot be overridden.
     * However, you can provide a custom implementation of `isFinishedFunction`.
     */
    autoscaledPoolOptions?: AutoscaledPoolOptions | undefined;
    /**
     * Sets the minimum concurrency (parallelism) for the crawl. Shortcut to the corresponding {@link AutoscaledPool } option.
     *
     * *WARNING:* If you set this value too high with respect to the available system memory and CPU, your crawler will run extremely slow or crash.
     * If you're not sure, just keep the default value and the concurrency will scale up automatically.
     */
    minConcurrency?: number | undefined;
    /**
     * Sets the maximum concurrency (parallelism) for the crawl. Shortcut to the corresponding {@link AutoscaledPool } option.
     */
    maxConcurrency?: number | undefined;
    /**
     * Basic crawler will initialize the  {@link SessionPool } with the corresponding `sessionPoolOptions`.
     * The session instance will be than available in the `handleRequestFunction`.
     */
    useSessionPool?: boolean | undefined;
    /**
     * The configuration options for {@link SessionPool } to use.
     */
    sessionPoolOptions?: SessionPoolOptions | undefined;
};
export type HandleRequest = (inputs: HandleRequestInputs) => Promise<void>;
export type HandleRequestInputs = {
    /**
     * The original {Request} object.
     * A reference to the underlying {@link AutoscaledPool } class that manages the concurrency of the crawler.
     * Note that this property is only initialized after calling the {@link BasicCrawlerrun } function.
     * You can use it to change the concurrency settings on the fly,
     * to pause the crawler by calling {@link AutoscaledPoolpause }
     * or to abort it by calling {@link AutoscaledPoolabort }.
     */
    request: Request;
    session?: Session | undefined;
    crawler?: BasicCrawler | undefined;
};
export type HandleFailedRequest = (inputs: HandleFailedRequestInput) => Promise<void>;
export type HandleFailedRequestInput = {
    /**
     * The Error thrown by `handleRequestFunction`.
     */
    error: Error;
    /**
     * The original {Request} object.
     */
    request: Request;
    session: Session;
    proxyInfo: ProxyInfo;
};
import { RequestList } from "../request_list";
import { RequestQueue } from "../storages/request_queue";
import Statistics from "./statistics";
import { SessionPoolOptions } from "../session_pool/session_pool";
import AutoscaledPool from "../autoscaling/autoscaled_pool";
import Request from "../request";
import { Session } from "../session_pool/session";
import { ProxyInfo } from "../proxy_configuration";
import { AutoscaledPoolOptions } from "../autoscaling/autoscaled_pool";
//# sourceMappingURL=basic_crawler.d.ts.map